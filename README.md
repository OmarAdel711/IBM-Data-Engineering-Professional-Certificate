# IBM Data Engineering Professional Certificate

> **Offered by:** IBM  
> **Platform:** Coursera  
> **Instructors:** IBM Skills Network Team, Muhammad Yahya, Abhishek Gagneja  
> **Level:** Beginner to Intermediate  
> **Duration:** ~6 months (10 hours/week)  
> **Credential:** IBM Professional Certificate + IBM Digital Badge  

---

## 🎯 About the Program

Prepare for a career as a **Data Engineer** and build **job-ready AI-powered skills** for one of the most in-demand roles in data today.  

This program covers everything from **data architecture, database systems, pipelines, and ETL processes** to **Big Data, Spark, Kafka, and Generative AI tools**.  
No prior experience is required — this is a hands-on, project-driven certification designed by IBM.

---

## 🧩 Program Learning Outcomes

- Design and manage **relational and non-relational databases** (MySQL, PostgreSQL, IBM Db2, MongoDB, Cassandra).  
- Implement **ETL pipelines** using **Bash, Airflow, and Kafka**.  
- Architect, populate, and query **data warehouses**; build BI dashboards using **IBM Cognos Analytics** and **Google Looker Studio**.  
- Work with **Big Data tools** like **Hadoop**, **Apache Spark**, **Spark SQL**, and **Spark ML**.  
- Apply **Generative AI** for data augmentation, warehouse schema generation, and automation.  
- Master **Linux commands, shell scripting, and database administration (DBA)** tasks.  
- Build a **capstone project** integrating multiple technologies and datasets.  

---

## 🧰 Skills Gained

- **Programming:** Python, SQL, Bash  
- **Database Systems:** RDBMS (MySQL, PostgreSQL, IBM Db2), NoSQL (MongoDB, Cassandra, Cloudant)  
- **ETL & Pipelines:** Airflow, Kafka, Shell scripting  
- **Big Data:** Apache Spark, Hadoop, PySpark  
- **Data Warehousing & BI:** IBM Cognos, Looker Studio, Schema Design  
- **AI & ML:** SparkML, Generative AI, Predictive Modeling  
- **Linux & DevOps:** Command-line, automation, environment setup  

---

## 🧱 Courses Overview

| # | Course | Duration | Key Topics |
|---|---------|-----------|------------|
| 1 | Introduction to Data Engineering | 13 hrs | Data lifecycle, pipelines, governance |
| 2 | Python for Data Science, AI & Development | 25 hrs | Python, Pandas, APIs, Jupyter |
| 3 | Python Project for Data Engineering | 9 hrs | ETL, Web Scraping, Data Transformation |
| 4 | Introduction to Relational Databases (RDBMS) | 15 hrs | ERD, MySQL, PostgreSQL, IBM Db2 |
| 5 | Databases and SQL for Data Science with Python | 18 hrs | SQL queries, DDL/DML, joins, stored procedures |
| 6 | Hands-on Introduction to Linux Commands and Shell Scripting | 14 hrs | Bash, file systems, cron jobs |
| 7 | Relational Database Administration (DBA) | 21 hrs | Backup, roles, optimization |
| 8 | ETL and Data Pipelines with Shell, Airflow and Kafka | 17 hrs | ETL, ELT, workflows, concurrency |
| 9 | Data Warehouse Fundamentals | 15 hrs | Schema design, aggregation, Cognos |
| 10 | BI Dashboards with IBM Cognos Analytics and Google Looker | 11 hrs | Visualization, dashboards |
| 11 | Introduction to NoSQL Databases | 18 hrs | MongoDB, Cassandra, CRUD |
| 12 | Introduction to Big Data with Spark and Hadoop | 19 hrs | Spark, Hadoop, HDFS, Hive |
| 13 | Machine Learning with Apache Spark | 15 hrs | SparkML, regression, clustering |
| 14 | Data Engineering Capstone Project | 17 hrs | End-to-end architecture |
| 15 | Generative AI: Elevate your Data Engineering Career | 13 hrs | Data generation, augmentation, anonymization |
| 16 | Data Engineering Career Guide & Interview Preparation | 11 hrs | Resume, portfolio, interview skills |

---

## 🧪 Applied Learning Projects

Throughout the program, I completed several hands-on projects, including:

- 🏢 Designing a **relational database** for a coffee franchise  
- 🧮 Querying **census and crime datasets** using SQL and Python  
- 🐧 Writing **Bash shell scripts** for automated file backups  
- ⚙️ Creating an **ETL pipeline** with Airflow and Kafka  
- 🧰 Designing a **data warehouse** for a waste management company  
- 🧠 Building an **ML pipeline** using Spark and SparkML  
- 🤖 Applying **Generative AI** to design data schemas and automate ETL tasks  
- 💡 Developing a **capstone project** integrating databases, Spark, and BI tools  

---

## 🧱 Repository Structure

IBM-Data-Engineering-Professional-Certificate/
│
├── 01-Introduction-to-Data-Engineering/
├── 02-Python-for-Data-Science/
├── 03-Python-Project-for-Data-Engineering/
├── 04-Relational-Databases/
├── 05-SQL-for-Data-Science/
├── 06-Linux-and-Shell/
├── 07-Database-Administration/
├── 08-ETL-and-Pipelines/
├── 09-Data-Warehouse/
├── 10-BI-Dashboards/
├── 11-NoSQL/
├── 12-Big-Data/
├── 13-ML-with-Spark/
├── 14-Capstone-Project/
├── 15-Generative-AI/
└── 16-Career-Guide/

Each folder contains:
- Labs (`.ipynb`)
- Scripts (`.py`)
- SQL queries (`.sql`)
- Mini projects or assignments  

---

## 🏅 Credentials

- 🎓 **IBM Data Engineering Professional Certificate**  
- 🪪 **IBM Digital Badge** (Data Engineering)  
- 🌐 [Verify Credential on Coursera](https://www.coursera.org/professional-certificates/ibm-data-engineer)

---

## 👤 Author

**Omar Adel Abdelsalam Elsheikh**  
📧 [omaraadel711@gmail.com](mailto:omaraadel711@gmail.com)  
🔗 [LinkedIn](https://www.linkedin.com/in/omar-adel-ai/)  
💻 [GitHub](https://github.com/OmarAdel711)

---

> 💡 *This repository serves as a portfolio of my work and learning through the IBM Data Engineering Professional Certificate. It demonstrates proficiency in modern data engineering tools, platforms, and AI integration.*


